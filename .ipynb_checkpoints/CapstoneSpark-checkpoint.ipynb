{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ab202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f2cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c806983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading traffic-violations-in-usa.zip to C:\\Users\\KEVAL\\Desktop\\DataEngineering_CapstoneProject\n",
      "\n",
      "downloaded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/48.5M [00:00<?, ?B/s]\n",
      "  2%|2         | 1.00M/48.5M [00:00<00:05, 8.67MB/s]\n",
      "  6%|6         | 3.00M/48.5M [00:00<00:03, 14.8MB/s]\n",
      " 10%|#         | 5.00M/48.5M [00:00<00:02, 16.5MB/s]\n",
      " 14%|#4        | 7.00M/48.5M [00:00<00:02, 18.0MB/s]\n",
      " 19%|#8        | 9.00M/48.5M [00:00<00:02, 18.6MB/s]\n",
      " 25%|##4       | 12.0M/48.5M [00:00<00:01, 19.7MB/s]\n",
      " 29%|##8       | 14.0M/48.5M [00:05<00:23, 1.52MB/s]\n",
      " 33%|###2      | 16.0M/48.5M [00:05<00:16, 2.06MB/s]\n",
      " 37%|###7      | 18.0M/48.5M [00:05<00:12, 2.62MB/s]\n",
      " 41%|####1     | 20.0M/48.5M [00:05<00:09, 3.31MB/s]\n",
      " 43%|####3     | 21.0M/48.5M [00:05<00:07, 3.65MB/s]\n",
      " 47%|####7     | 23.0M/48.5M [00:06<00:05, 4.84MB/s]\n",
      " 52%|#####1    | 25.0M/48.5M [00:06<00:03, 6.36MB/s]\n",
      " 56%|#####5    | 27.0M/48.5M [00:06<00:02, 8.11MB/s]\n",
      " 62%|######1   | 30.0M/48.5M [00:06<00:01, 10.7MB/s]\n",
      " 66%|######5   | 32.0M/48.5M [00:06<00:01, 12.0MB/s]\n",
      " 72%|#######2  | 35.0M/48.5M [00:06<00:01, 14.1MB/s]\n",
      " 76%|#######6  | 37.0M/48.5M [00:06<00:00, 14.5MB/s]\n",
      " 82%|########2 | 40.0M/48.5M [00:06<00:00, 16.6MB/s]\n",
      " 89%|########8 | 43.0M/48.5M [00:07<00:00, 18.4MB/s]\n",
      " 95%|#########4| 46.0M/48.5M [00:07<00:00, 17.6MB/s]\n",
      " 99%|#########8| 48.0M/48.5M [00:07<00:00, 15.8MB/s]\n",
      "100%|##########| 48.5M/48.5M [00:07<00:00, 6.75MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzipped the data\n"
     ]
    }
   ],
   "source": [
    "#Download Dataset and Extract on local Machine\n",
    "!kaggle datasets download --force -d felix4guti/traffic-violations-in-usa\n",
    "print('downloaded data')\n",
    "with zipfile.ZipFile(\"./{}.zip\".format(\"traffic-violations-in-usa\"),\"r\") as z:\n",
    "    z.extractall(\".\")\n",
    "print('unzipped the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d62fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark \n",
    "conf = pyspark.SparkConf().setAppName('SparkApp').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1664b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ecd074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date Of Stop: string (nullable = true)\n",
      " |-- Time Of Stop: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- SubAgency: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Accident: string (nullable = true)\n",
      " |-- Belts: string (nullable = true)\n",
      " |-- Personal Injury: string (nullable = true)\n",
      " |-- Property Damage: string (nullable = true)\n",
      " |-- Fatal: string (nullable = true)\n",
      " |-- Commercial License: string (nullable = true)\n",
      " |-- HAZMAT: string (nullable = true)\n",
      " |-- Commercial Vehicle: string (nullable = true)\n",
      " |-- Alcohol: string (nullable = true)\n",
      " |-- Work Zone: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- VehicleType: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Violation Type: string (nullable = true)\n",
      " |-- Charge: string (nullable = true)\n",
      " |-- Article: string (nullable = true)\n",
      " |-- Contributed To Accident: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Driver City: string (nullable = true)\n",
      " |-- Driver State: string (nullable = true)\n",
      " |-- DL State: string (nullable = true)\n",
      " |-- Arrest Type: string (nullable = true)\n",
      " |-- Geolocation: string (nullable = true)\n",
      "\n",
      "+------------+------------+------+--------------------+--------------------+--------------------+----------------+-----------------+--------+-----+---------------+---------------+-----+------------------+------+------------------+-------+---------+-----+--------------------+----+---------+--------+------+--------------+-------------+--------------------+-----------------------+--------+------+--------------+------------+--------+-----------------+--------------------+\n",
      "|Date_Of_Stop|Time_Of_Stop|Agency|           SubAgency|         Description|            Location|        Latitude|        Longitude|Accident|Belts|Personal_Injury|Property_Damage|Fatal|Commercial_License|HAZMAT|Commercial_Vehicle|Alcohol|Work_Zone|State|         VehicleType|Year|     Make|   Model| Color|Violation_Type|       Charge|             Article|Contributed_To_Accident|    Race|Gender|   Driver_City|Driver_State|DL_State|      Arrest_Type|         Geolocation|\n",
      "+------------+------------+------+--------------------+--------------------+--------------------+----------------+-----------------+--------+-----+---------------+---------------+-----+------------------+------+------------------+-------+---------+-----+--------------------+----+---------+--------+------+--------------+-------------+--------------------+-----------------------+--------+------+--------------+------------+--------+-----------------+--------------------+\n",
      "|  09/24/2013|    17:11:00|   MCP|3rd district, Sil...|DRIVING VEHICLE O...|     8804 FLOWER AVE|            null|             null|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2008|     FORD|      4S| BLACK|      Citation|    13-401(h)|Transportation Ar...|                     No|   BLACK|     M|   TAKOMA PARK|          MD|      MD|A - Marked Patrol|                null|\n",
      "|  12/20/2012|    00:41:00|   MCP|2nd district, Bet...|DRIVING WHILE IMP...|NORFOLK AVE /  ST...|      38.9835782|     -77.09310515|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2005|     AUDI|      4S|  GRAY|      Citation|   21-902(b1)|Transportation Ar...|                     No|   WHITE|     M|       DERWOOD|          MD|      MD|A - Marked Patrol|(38.9835782, -77....|\n",
      "|  07/20/2012|    23:12:00|   MCP|5th district, Ger...|FAILURE TO STOP A...|WISTERIA DR @ WAR...|39.1618098166667|     -77.25358095|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2002|     TOYT|      4S|   RED|      Citation|    21-707(a)|Transportation Ar...|                     No|   ASIAN|     F|    GERMANTOWN|          MD|      MD|A - Marked Patrol|(39.1618098166667...|\n",
      "|  03/19/2012|    16:10:00|   MCP|2nd district, Bet...|DRIVER USING HAND...|CLARENDON RD @ EL...|38.9827307333333|-77.1007551666667|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   VA|     02 - Automobile|1996|    HONDA|   CIVIC|SILVER|      Citation|21-1124.2(d2)|Transportation Ar...|                     No|HISPANIC|     M|     ARLINGTON|          VA|      VA|A - Marked Patrol|(38.9827307333333...|\n",
      "|  12/01/2014|    12:52:00|   MCP|6th district, Gai...|FAILURE STOP AND ...|CHRISTOPHER AVE/M...|39.1628883333333|-77.2290883333333|      No|   No|             No|            Yes|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2001|    HONDA|  ACCORD|SILVER|      Citation|    21-403(b)|Transportation Ar...|                     No|   BLACK|     F|UPPER MARLBORO|          MD|      MD|A - Marked Patrol|(39.1628883333333...|\n",
      "|  06/09/2012|    21:19:00|   MCP|3rd district, Sil...|OCCUPANT UNDER 16...|2068 HARLEQUIN TE...|     39.06914295|-76.9696780666667|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2004|CHEVROLET|  IMPALA|SILVER|      Citation|  22-412.3(b)|Transportation Ar...|                     No|   WHITE|     F| SILVER SPRING|          MD|      MD|A - Marked Patrol|(39.06914295, -76...|\n",
      "|  09/11/2012|    21:47:00|   MCP|4th district, Whe...|PERSON DRIVING MO...|TWIG RD AT GOOD H...|     39.09619885|     -76.98696215|      No|  Yes|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2009|    DODGE| CHARGER| BLACK|      Citation|    16-303(d)|Transportation Ar...|                     No|   BLACK|     M| SILVER SPRING|          MD|      MD|A - Marked Patrol|(39.09619885, -76...|\n",
      "|  11/04/2012|    01:14:00|   MCP|4th district, Whe...|DRIVING UNDER THE...|RANDOLPH RD @ SPR...|      39.0583676|-77.0479825666667|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2003|     FORD|  TAURUS| WHITE|      Citation|   21-902(a2)|Transportation Ar...|                     No|   BLACK|     M| SILVER SPRING|          MD|      MD|A - Marked Patrol|(39.0583676, -77....|\n",
      "|  08/27/2012|    00:04:00|   MCP|5th district, Ger...|PERSON DRIVING MO...|GERMANTOWN RD@CLO...|39.1612861666667|-77.2837891833333|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2004| INFINITY|     QX4|SILVER|      Citation|    16-303(h)|Transportation Ar...|                     No|   WHITE|     M|    GERMANTOWN|          MD|      MD|A - Marked Patrol|(39.1612861666667...|\n",
      "|  05/21/2012|    16:47:00|   MCP|2nd district, Bet...|OPERATOR NOT REST...|CLUB DR/CONNECTIC...|      38.9893187|      -77.0753899|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|05 - Light Duty T...|2005|     TOYT|4 RUNNER| BLACK|      Citation|  22-412.3(b)|Transportation Ar...|                     No|   BLACK|     M|  OWINGS MILLS|          MD|      MD|A - Marked Patrol|(38.9893187, -77....|\n",
      "|  08/27/2013|    00:55:00|   MCP|2nd district, Bet...|NEGLIGENT DRIVING...|CONNECTICUT/CHEVY...|            null|             null|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2013|  HYUNDAI| ELANTRA|  GRAY|      Citation|  21-901.1(b)|Transportation Ar...|                     No|   WHITE|     F| SILVER SPRING|          MD|      MD|A - Marked Patrol|                null|\n",
      "|  10/08/2013|    13:23:00|   MCP|4th district, Whe...|DRIVING VEHICLE O...|GEORGIA AVE / BEL...|39.0933833333333|-77.0795516666667|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|1993|     FORD|  PICKUP| BLACK|      Citation|    13-401(h)|Transportation Ar...|                     No|HISPANIC|     M|    BELTSVILLE|          MD|      MD|A - Marked Patrol|(39.0933833333333...|\n",
      "|  04/24/2015|    00:38:00|   MCP|1st district, Roc...|DRIVER FAIL TO ST...|EB MONTROSE PKWY/...|            null|             null|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   VA|     02 - Automobile|2003|    DODGE|SPRINTER| WHITE|      Citation|    21-204(b)|Transportation Ar...|                     No|HISPANIC|     M| SILVER SPRING|          MD|      MD|A - Marked Patrol|                null|\n",
      "|  01/12/2014|    20:56:00|   MCP|1st district, Roc...|PERSON DRIVING MO...|199 E. JEFFERSON ST.|39.0694816666667|       -77.162145|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2013|   NISSAN|  SENTRA|  GRAY|      Citation|    16-303(h)|Transportation Ar...|                     No|HISPANIC|     M| SILVER SPRING|          MD|      MD|A - Marked Patrol|(39.0694816666667...|\n",
      "|  02/14/2014|    20:10:00|   MCP|1st district, Roc...|FAILURE TO DRIVE ...|GATEWAY CENTER DR...|39.2348434333333|     -77.28153995|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2005|     CADI|     STS| BLACK|      Citation|    21-311(1)|Transportation Ar...|                     No|   WHITE|     M| POINT OF ROCK|          MD|      WV|A - Marked Patrol|(39.2348434333333...|\n",
      "|  04/09/2014|    22:22:00|   MCP|3rd district, Sil...|DRIVING VEHICLE O...|MAHAN RD ON NEW H...|        39.03408|       -76.986785|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2004|CHEVROLET|  IMPALA| WHITE|      Citation|    16-115(g)|Transportation Ar...|                     No|   BLACK|     M|      LANDOVER|          MD|      MD|A - Marked Patrol|(39.03408, -76.98...|\n",
      "|  12/06/2012|    12:34:00|   MCP|Headquarters and ...|PEDESTRIAN CROSSI...|FLOWER AVE @ PINE...|      38.9984182|     -77.00384695|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   XX|        29 - Unknown|null|     NONE|    NONE|   N/A|      Citation|    21-503(c)|Transportation Ar...|                     No|HISPANIC|     M| SILVER SPRING|          MD|      MD|  O - Foot Patrol|(38.9984182, -77....|\n",
      "|  12/07/2012|    18:36:00|   MCP|1st district, Roc...|DRIVING VEHICLE W...| RT 255/REDLAND ROAD|39.1154527666667|-77.1658159833333|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2011|   TOYOTA|  TACOMA| GREEN|      Citation|   21-902(a1)|Transportation Ar...|                    Yes|   WHITE|     M|     BALTIMORE|          MD|      MD|A - Marked Patrol|(39.1154527666667...|\n",
      "|  09/22/2012|    21:24:00|   MCP|3rd district, Sil...|DRIVER FAILURE TO...|29 N/B AT FAIRLAN...|39.0067167333333|      -76.8914382|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   VA|     02 - Automobile|2013| CADILLAC|     CTS|SILVER|      Citation|   21-201(a1)|Transportation Ar...|                     No|   BLACK|     M|    WASHINGTON|          DC|      DC|A - Marked Patrol|(39.0067167333333...|\n",
      "|  07/23/2013|    23:15:00|   MCP|4th district, Whe...|FAILURE TO ATTACH...|108 / OLD BALTIMO...|39.1516659166667|      -77.0706278|      No|   No|             No|             No|   No|                No|    No|                No|     No|       No|   MD|     02 - Automobile|2000|   SATURN|      LS|SILVER|      Citation|    13-411(a)|Transportation Ar...|                     No|   BLACK|     F|    BELTSVILLE|          MD|      MD|A - Marked Patrol|(39.1516659166667...|\n",
      "+------------+------------+------+--------------------+--------------------+--------------------+----------------+-----------------+--------+-----+---------------+---------------+-----+------------------+------+------------------+-------+---------+-----+--------------------+----+---------+--------+------+--------------+-------------+--------------------+-----------------------+--------+------+--------------+------------+--------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o107.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-db0766e8f89b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0msource_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#source_df.write.parquet(r\"C:/Users/KEVAL/Desktop/DataEngineering_CapstoneProject/ParquetFiles/Violations_traffic.parquet\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0msource_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:/Users/KEVAL/Desktop/DataEngineering_CapstoneProject/ParquetFiles/Violations.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o107.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "#Read data.csv into a RDD to dataframe\n",
    "#source_df = sc.textFile(r\"C:/Users/KEVAL/Desktop/DataEngineering_CapstoneProject/Traffic_Violations.csv\")\n",
    "\n",
    "source_df = spark.read.load(r'C:/Users/KEVAL/Desktop/DataEngineering_CapstoneProject/Traffic_Violations.csv', format =\"csv\", sep=\",\",inferSchema=True, header = True)\n",
    "\n",
    "source_df.printSchema()\n",
    "#source_df.show()\n",
    "\n",
    "#source_df.toDF().write.mode('overwrite').parquet(r\"C:/Users/KEVAL/Desktop/DataEngineering_CapstoneProject/ParquetFiles/Violations.parquet\")\n",
    "#source_df = spark.read.format(r\"C:\\Users\\KEVAL\\Desktop\\DataEngineering_CapstoneProject\\Traffic_Violations.csv\")\n",
    "#source_df.toDF().parquet(r\"C:\\Users\\KEVAL\\Desktop\\DataEngineering_CapstoneProject\\ParquetFiles\\Violations.parquet\")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "#Replace space \" \" with underscore\"_\"\n",
    "source_df = source_df.select([\n",
    "    col(c).alias(c.replace(\" \",\"_\"))\n",
    "    for c in source_df.columns\n",
    "])\n",
    "source_df.show()\n",
    "#source_df.write.parquet(r\"C:/Users/KEVAL/Desktop/DataEngineering_CapstoneProject/ParquetFiles/Violations_traffic.parquet\")\n",
    "source_df.write.mode('overwrite').parquet(r\"C:/Users/KEVAL/Desktop/DataEngineering_CapstoneProject/ParquetFiles/Violations.parquet\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f25449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657995b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a450ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
